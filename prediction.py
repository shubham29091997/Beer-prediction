# -*- coding: utf-8 -*-
"""prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-7QomT4L0--nzxOHJbM5ixI1rnyDc9W-
"""

import os 
import nltk
from nltk.corpus import stopwords
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.decomposition import TruncatedSVD
from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer

import pandas as pd
df = pd.read_csv('/content/train.csv')
df

df.shape

df.columns

df.describe()

df.info()

df1 = df.drop(['user/ageInSeconds', 'user/birthdayRaw', 'user/birthdayUnix',  'user/gender'], axis =1)
df1

df2 = df1.dropna()
df2

df2.info()

df2.describe()

df2['beer/ABV'].unique()

#df2['beer/ABV'].describe()
df2['beer/ABV'].value_counts()

df2.groupby('beer/ABV').size().sort_values().tail(15)

sns.distplot(df2.groupby('beer/ABV').size().sort_values())

df2['beer/beerId'].unique()

df2['beer/beerId'].value_counts()

df2.groupby('beer/beerId').size().sort_values().tail(5)

sns.distplot(df2.groupby('beer/beerId').size().sort_values())

df2['beer/brewerId'].unique()

df2['beer/brewerId'].value_counts()

sns.distplot(df2.groupby('beer/brewerId').size().sort_values())

df2['beer/name'].unique()

df2['beer/name'].value_counts()

sns.distplot(df2.groupby('beer/name').size().sort_values())

import spacy 
en_nlp = spacy.load('en')
def custom_tokenizer(document):
    doc_spacy = en_nlp(document)
    return [token.lemma_ for token in doc_spacy]

tfidf2 = TfidfVectorizer(strip_accents='unicode',lowercase=True,ngram_range=(1,2),max_df=0.7,min_df=5,tokenizer = custom_tokenizer) 

genre_sparse_matrix = tfidf2.fit_transform(df2['beer/name'])

ts2= TruncatedSVD(n_components = 3)
genre_features = ts2.fit_transform(genre_sparse_matrix)

#genre_cols = ['Genre'+str(i) for i in range(3)]
# genre_cols

df_new = pd.DataFrame(genre_features)

df_new

df2['beer/style'].unique()

df2['beer/style'].value_counts()

tfidf = TfidfVectorizer(strip_accents = 'unicode' , lowercase=True,ngram_range=(1,3),max_df=0.7,min_df=5,tokenizer=custom_tokenizer)
sparse_matrix = tfidf.fit_transform(df2['beer/style'])

ts = TruncatedSVD(n_components = 3)
style_features = ts.fit_transform(sparse_matrix)

df_new2 = pd.DataFrame(style_features)
df_new2

plt.scatter(df2['review/appearance'],df2['review/overall'])

column1 = df2['review/appearance']
column2 = df2['review/overall']
corelation = column2.corr(column1)
print(corelation)

plt.scatter(df2['review/aroma'],df2['review/overall'])

column1 = df2['review/aroma']
column2 = df2['review/overall']
corelation = column2.corr(column1)
print(corelation)

plt.scatter(df2['review/palate'],df2['review/overall'])

column1 = df2['review/palate']
column2 = df2['review/overall']
corelation = column2.corr(column1)
print(corelation)

plt.scatter(df2['review/taste'],df2['review/overall'])

column1 = df2['review/taste']
column2 = df2['review/overall']
corelation = column2.corr(column1)
print(corelation)

import string 
string.punctuation

def remove_punc(text):
  txt_nopunc = "".join([c for c in text if c not in string.punctuation])
  return txt_nopunc

df4 = df2['review/text'].apply(lambda x : remove_punc(x) )
 df4

import re 

def tokenize(text):
  tokens = re.split('\W+' , text)
  return tokens
df5 = df4.apply(lambda x : tokenize(x.lower()))
df5

import nltk
nltk.download("stopwords")

import nltk
stopwords = nltk.corpus.stopwords.words('english')

def remove_stopwords(text_tokenized):
  text_clean = [word for word in text_tokenized if word not in stopwords]
  return text_clean

df6 = df5.apply(lambda x : remove_stopwords(x))  
df6

tfidf_text = TfidfVectorizer(strip_accents = 'unicode' , lowercase=True,ngram_range=(1,3),max_df=0.7,min_df=5,tokenizer=custom_tokenizer)
matrix = tfidf_text.fit_transform(df2['review/text'])

ts3 = TruncatedSVD(n_components = 15)
review_features = ts3.fit_transform(matrix)

df_new3 = pd.DataFrame(review_features)
df_new3

df2['user/profileName'].unique()

df2['user/profileName'].value_counts()

df2["review/taste"]

frames = [df_new, df_new2 , df_new3, df2['review/taste'] ,df2['review/appearance'] , df2['review/aroma'] , df2['review/palate'] ,df2['review/overall'] ]
result = pd.concat(frames, axis=1)
result

features = result.dropna()
features

x = features.drop('review/overall' , axis =1).values
y = features['review/overall'].values

from sklearn.preprocessing import scale
x_scaled = scale(x)
y_scaled = scale(y)

from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split

x_train , x_test , y_train, y_test = train_test_split(x,y, test_size = 0.3, random_state = 42)

reg_all = LinearRegression()
reg_all.fit(x_train , y_train)
y_pred = reg_all.predict(x_test)
reg_all.score(x_test , y_test)

from sklearn.tree import DecisionTreeRegressor
x_train , x_test , y_train, y_test = train_test_split(x,y, test_size = 0.3, random_state = 42)
dt = DecisionTreeRegressor(max_depth=5,  random_state=6)
dt.fit(x_train , y_train)
y_pred = dt.predict(x_test)
dt.score(x_test , y_test)

from sklearn import metrics
print(metrics.mean_absolute_error(y_test, y_pred))
print(metrics.mean_squared_error(y_test, y_pred))

print(metrics.mean_squared_log_error(y_test, y_pred))

from sklearn.model_selection import cross_val_score
cv_results = cross_val_score(reg_all , x, y,  cv=5 )
print(cv_results)

